{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import BDay\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Outliers in last 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching data:\", response.status_code, response.text)\n",
    "        return None\n",
    "    data = response.json()\n",
    "    if 'results' not in data:\n",
    "        print(\"No 'results' key in response:\", data)\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "def calculate_daily_returns(df, prev_close=None):\n",
    "    if prev_close is not None:\n",
    "        df.loc[df.index[0], 'prev_close'] = prev_close\n",
    "    else:\n",
    "        df['prev_close'] = df['c'].shift(1)\n",
    "    df['daily_return'] = (df['c'] - df['prev_close']) / df['prev_close']\n",
    "    df['abs_daily_return'] = df['daily_return'].abs()\n",
    "    return df\n",
    "\n",
    "def get_top_outliers(df, n=10):\n",
    "    return df.nlargest(n, 'abs_daily_return')\n",
    "\n",
    "def update_outliers_list(current_df, historical_outliers_df, real_time_outliers_df, n=10):\n",
    "    if 'source' not in current_df.columns:\n",
    "        current_df['source'] = 'real-time'\n",
    "    combined_df = pd.concat([historical_outliers_df, current_df])\n",
    "    updated_outliers_df = combined_df.nlargest(n, 'abs_daily_return')\n",
    "    updated_historical_outliers_df = updated_outliers_df[updated_outliers_df['source'] == 'historical']\n",
    "    updated_real_time_outliers_df = updated_outliers_df[updated_outliers_df['source'] == 'real-time']\n",
    "    return updated_historical_outliers_df, updated_real_time_outliers_df\n",
    "\n",
    "def convert_timestamps(df):\n",
    "    df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "    df.drop(columns=['t'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# API key and endpoints\n",
    "api_key = 'beBybSi8daPgsTp5yx5cHtHpYcrjp5Jq'\n",
    "today = pd.Timestamp.now().date()\n",
    "start_date = today - pd.DateOffset(years=1)\n",
    "start_date_formatted = start_date.strftime('%Y-%m-%d')\n",
    "end_date = today - pd.DateOffset(days=1)\n",
    "end_date_formatted = end_date.strftime('%Y-%m-%d')\n",
    "symbol = 'C:USDCHF'\n",
    "historical_url = f'https://api.polygon.io/v2/aggs/ticker/{symbol}/range/1/day/{start_date_formatted}/{end_date_formatted}?adjusted=true&sort=asc&apiKey={api_key}'\n",
    "real_time_url = f'https://api.polygon.io/v2/aggs/ticker/{symbol}/range/1/day/{today}/{today}?adjusted=true&sort=asc&apiKey={api_key}'\n",
    "\n",
    "# Fetch and process historical data\n",
    "historical_data = fetch_data(historical_url)\n",
    "if historical_data:\n",
    "    historical_df = pd.DataFrame(historical_data['results'])\n",
    "    historical_df = convert_timestamps(historical_df)\n",
    "    historical_df = calculate_daily_returns(historical_df)\n",
    "    historical_df['source'] = 'historical'\n",
    "    historical_outliers_df = get_top_outliers(historical_df)\n",
    "else:\n",
    "    print(\"Failed to fetch or process historical data.\")\n",
    "\n",
    "# Fetch and process real-time data\n",
    "real_time_data = fetch_data(real_time_url)\n",
    "if real_time_data and 'results' in real_time_data:\n",
    "    real_time_df = pd.DataFrame(real_time_data['results'])\n",
    "    real_time_df = convert_timestamps(real_time_df)\n",
    "    # Use the last close from historical data\n",
    "    last_close = historical_df['c'].iloc[-1] if not historical_df.empty else None\n",
    "    real_time_df = calculate_daily_returns(real_time_df, prev_close=last_close)\n",
    "    real_time_df['source'] = 'real-time'\n",
    "    updated_historical_outliers_df, updated_real_time_outliers_df = update_outliers_list(real_time_df, historical_outliers_df, pd.DataFrame())\n",
    "    # Update historical data\n",
    "    historical_df = pd.concat([historical_df.iloc[1:], real_time_df])  # Keep historical data rolling\n",
    "else:\n",
    "    print(\"No new data available or failed to fetch real-time data.\")\n",
    "    \n",
    "# Combine data for Top 10 Outliers\n",
    "full_outlier_df = pd.concat([updated_historical_outliers_df, updated_real_time_outliers_df])\n",
    "\n",
    "# Print the Outliers\n",
    "full_outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_outliers_data = full_outlier_df.sort_values(by=\"date\")\n",
    "sorted_outliers_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Hourly data for 3 days prior and post outlier days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in dataset to datetime objects\n",
    "sorted_outliers_data['date'] = pd.to_datetime(sorted_outliers_data['date'])\n",
    "\n",
    "date_ranges = pd.DataFrame({\n",
    "    \"start_date\": sorted_outliers_data['date'] - BDay(10),\n",
    "    \"end_date\": sorted_outliers_data['date'] + BDay(11),\n",
    "    \"outlier_date\": sorted_outliers_data['date'],\n",
    "})\n",
    "\n",
    "date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_returns(df, prev_close=None):\n",
    "    if prev_close is not None:\n",
    "        df.loc[df.index[0], 'prev_close'] = prev_close\n",
    "    else:\n",
    "        df['prev_close'] = df['c'].shift(1)\n",
    "    df['returns'] = (df['c'] - df['prev_close']) / df['prev_close']\n",
    "    return df\n",
    "\n",
    "def fetch_hourly_data_chunk(symbol, start_date, end_date, api_key):\n",
    "    formatted_start_date = start_date.strftime('%Y-%m-%d')\n",
    "    formatted_end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/range/1/hour/{formatted_start_date}/{formatted_end_date}?apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    \n",
    "    response_data = response.json()\n",
    "    \n",
    "    if 'results' not in response_data:\n",
    "        print(f\"No 'results' in response: {response_data}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(response_data['results'])\n",
    "    df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "    df.drop(columns=['t'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_and_process_hourly_data(symbol, start_date, end_date, api_key):\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Split the date range into smaller chunks\n",
    "    chunk_size = 3  # Fetch data in 7-day chunks\n",
    "    date_ranges = [(start_date + timedelta(days=i*chunk_size), \n",
    "                    min(end_date, start_date + timedelta(days=(i+1)*chunk_size - 1)))\n",
    "                   for i in range((end_date - start_date).days // chunk_size + 1)]\n",
    "\n",
    "    # print((end_date - start_date).days // chunk_size + 1)\n",
    "    all_data = []\n",
    "\n",
    "    for start, end in date_ranges:\n",
    "        chunk_data = fetch_hourly_data_chunk(symbol, start, end, api_key)\n",
    "        if chunk_data is not None:\n",
    "            all_data.append(chunk_data)\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data fetched\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.concat(all_data)\n",
    "    hourly_data = calculate_daily_returns(df)\n",
    "    hourly_data.set_index('date', inplace=True)\n",
    "    \n",
    "    full_index = pd.date_range(start=start_date, end=end_date + timedelta(days=1), freq='H')\n",
    "    hourly_data = hourly_data.reindex(full_index)\n",
    "    \n",
    "    hourly_data.reset_index(inplace=True)\n",
    "    hourly_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "    \n",
    "    return hourly_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Initialize empty DataFrame\n",
    "# all_data = pd.DataFrame()\n",
    "\n",
    "# # Initialize an outlier identifier starting from 1 or any specific number\n",
    "# outlier_id = 1\n",
    "\n",
    "# for index, row in date_ranges.iterrows():\n",
    "#     # Convert start_date, end_date, and outlier_date to Timestamp for consistent comparison\n",
    "#     start_date_ts = pd.Timestamp(row['start_date'])\n",
    "#     end_date_ts = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)  # Extend the end date by one additional day\n",
    "#     outlier_date_ts = pd.Timestamp(row['outlier_date'])\n",
    "    \n",
    "#     # Get hourly data for the range including 3 days before and after the outlier\n",
    "#     hourly_data = fetch_and_process_hourly_data(symbol, start_date_ts, end_date_ts, api_key)\n",
    "#     # print(hourly_data)\n",
    "#     # Check if hourly_data is not None before processing\n",
    "#     if hourly_data is not None:\n",
    "#         # Assign the current outlier_id to the data\n",
    "#         hourly_data['outlier_id'] = outlier_id\n",
    "\n",
    "#         # Filter out weekdends\n",
    "#         hourly_data = hourly_data[~hourly_data['date'].dt.weekday.isin([5,6])]\n",
    "        \n",
    "#         # prior_data from start_date to outlier_date inclusive\n",
    "#         prior_data = hourly_data[(hourly_data['date'] >= start_date_ts) & (hourly_data['date'] < outlier_date_ts)]\n",
    "#         prior_data[\"day type\"] = \"prior day\"\n",
    "\n",
    "#         # outlier_data is for the hourly data on the day of the outlier\n",
    "#         outlier_data = hourly_data[(hourly_data['date'].dt.date == outlier_date_ts.date())]\n",
    "#         outlier_data[\"day type\"] = \"outlier day\"\n",
    "        \n",
    "#         # post_data from the day after outlier_date to end_date\n",
    "#         post_outlier_ts = outlier_date_ts + pd.Timedelta(days=1)  # Starting the day after the outlier_date\n",
    "#         post_data = hourly_data[(hourly_data['date'] > post_outlier_ts) & (hourly_data['date'] <= end_date_ts)]\n",
    "#         post_data[\"day type\"] = \"post day\"\n",
    "#         # print(post_outlier_ts)\n",
    "#         # print(end_date_ts)\n",
    "#         # print(post_data)\n",
    "        \n",
    "#         # Concatenate the data from this iteration to the cumulative DataFrame\n",
    "#         all_data = pd.concat([all_data, prior_data, outlier_data, post_data])\n",
    "\n",
    "#         # Increment the outlier_id for the next iteration\n",
    "#         outlier_id += 1\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Data not available for symbol {symbol} from {row['start_date'].date()} to {row['end_date'].date()}\")\n",
    "\n",
    "# # Add the day column to the final DataFrame\n",
    "# all_data['day'] = all_data['date'].dt.day_name()\n",
    "# # Optionally, you can reset the index of the final DataFrame if it becomes non-unique after concatenations\n",
    "# all_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating and comparing similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def dtw_distance(series1, series2):\n",
    "#     n, m = len(series1), len(series2)\n",
    "#     dtw_matrix = np.zeros((n+1, m+1))\n",
    "    \n",
    "#     for i in range(n+1):\n",
    "#         for j in range(m+1):\n",
    "#             dtw_matrix[i, j] = np.inf\n",
    "#     dtw_matrix[0, 0] = 0\n",
    "    \n",
    "#     for i in range(1, n+1):\n",
    "#         for j in range(1, m+1):\n",
    "#             cost = abs(series1[i-1] - series2[j-1])\n",
    "#             # Take last min from a square sub-matrix\n",
    "#             last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n",
    "#             dtw_matrix[i, j] = cost + last_min\n",
    "            \n",
    "#     return dtw_matrix[n, m]\n",
    "\n",
    "# # Load your data\n",
    "# data = pd.read_csv('USDCHF_hourly.csv')\n",
    "# # Fill null values with ffill then bfill to ensure all nulls are handled\n",
    "# data.fillna(method='ffill', inplace=True)\n",
    "# data.fillna(method='bfill', inplace=True)\n",
    "# all_data.fillna(method='ffill', inplace=True)\n",
    "# all_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# # Filter Prior Day Data\n",
    "# prior_day_data = data[data['day type'] == 'prior day']\n",
    "\n",
    "# # # Main series to compare others against\n",
    "# main = all_data[all_data['day type'] == 'prior day']\n",
    "# ts_main_id = main[main['outlier_id'] == 10]['c'].values\n",
    "\n",
    "# # Calculate DTW distances between the current outlier's series and each of the other outliers\n",
    "# dtw_distances = {}\n",
    "# for id in prior_day_data[\"outlier_id\"].unique():\n",
    "#     series = prior_day_data[prior_day_data['outlier_id'] == id]['c'].values\n",
    "#     distance = dtw_distance(ts_main_id, series)\n",
    "#     dtw_distances[id] = distance\n",
    "\n",
    "# # Sort the distances and get the top 10 lowest\n",
    "# top_10_ids = sorted(dtw_distances, key=dtw_distances.get)[:10]\n",
    "\n",
    "# lowest_dtw = 99999\n",
    "\n",
    "# # Output the results\n",
    "# for id in top_10_ids:\n",
    "#     print(f\"DTW Distance between Current Outlier and ID {id}: {dtw_distances[id]:.2f}\")\n",
    "#     if dtw_distances[id] < lowest_dtw:\n",
    "#         lowest_dtw = dtw_distances[id]\n",
    "#         best_outlier_match_id = id\n",
    "#     # best_outlier_match_id = min(best_outlier_match_id, dtw_distances[id])\n",
    "\n",
    "# print(best_outlier_match_id)\n",
    "# data_aug = data[data['outlier_id'] == best_outlier_match_id]\n",
    "# print(len(data_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_aug.drop(columns=[\"Unnamed: 0\", \"n\", \"prev_close\", \"returns\", \"outlier_id\", \"day\"], inplace=True)\n",
    "# # Reset the index\n",
    "# data_aug.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data for the choosen historical outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the autoencoder model\n",
    "# def create_autoencoder(input_dim):\n",
    "#     input_layer = Input(shape=(input_dim,))\n",
    "#     encoded = Dense(64, activation='relu')(input_layer)\n",
    "#     encoded = Dense(32, activation='relu')(encoded)\n",
    "#     decoded = Dense(64, activation='relu')(encoded)\n",
    "#     output_layer = Dense(input_dim)(decoded)\n",
    "#     autoencoder = Model(input_layer, output_layer)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     return autoencoder\n",
    "\n",
    "# # Prepare the data\n",
    "# scaler = MinMaxScaler()\n",
    "# data_scaled = scaler.fit_transform(data_aug[['c']])\n",
    "\n",
    "# # Train the autoencoder\n",
    "# autoencoder = create_autoencoder(data_scaled.shape[1])\n",
    "# autoencoder.fit(data_scaled, data_scaled, epochs=100, batch_size=32, shuffle=True, verbose=1)\n",
    "\n",
    "# # Generate synthetic data\n",
    "# synthetic_data = []\n",
    "# for _ in range(50):\n",
    "#     noise = np.random.normal(0, 0.1, data_scaled.shape)\n",
    "#     synthetic_data_scaled = autoencoder.predict(data_scaled + noise)\n",
    "#     synthetic_data.append(scaler.inverse_transform(synthetic_data_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an outlier identifier starting from 1 or any specific number\n",
    "# outlier_id = 1\n",
    "\n",
    "# # Initialize an empty list to hold the individual synthetic dataframes\n",
    "# synthetic_dfs = []\n",
    "\n",
    "# # Iterate over each synthetic dataset\n",
    "# for i, synthetic in enumerate(synthetic_data):\n",
    "#     # Create a DataFrame from the synthetic data with a single column 'c'\n",
    "#     df = pd.DataFrame(synthetic, columns=['c'])\n",
    "#     # Add the 'day type' column from the existing 'data' DataFrame\n",
    "#     df['day type'] = data['day type']\n",
    "#     # Add the outlier_id column\n",
    "#     df['outlier_id'] = outlier_id\n",
    "#     # Increment the outlier_id for the next iteration\n",
    "#     outlier_id += 1\n",
    "#     # Append the DataFrame to the list\n",
    "#     synthetic_dfs.append(df)\n",
    "\n",
    "# # Concatenate all the individual DataFrames into a single DataFrame\n",
    "# combined_df = pd.concat(synthetic_dfs, ignore_index=True)\n",
    "\n",
    "# # Create a DataFrame from data_aug with outlier_id = 0\n",
    "# data_aug_df = data_aug[['c', 'day type']].copy()\n",
    "# data_aug_df['outlier_id'] = 0\n",
    "\n",
    "# # Concatenate the synthetic DataFrames with the data_aug DataFrame\n",
    "# df_train = pd.concat([data_aug_df, combined_df], ignore_index=True)\n",
    "\n",
    "# # Rename columns to match the desired format\n",
    "# df_train.columns = ['c', 'day_type', 'outlier_id']\n",
    "\n",
    "# # Reorder the columns to have 'c', 'outlier_id', 'day_type'\n",
    "# df_train = df_train[['c', 'outlier_id', 'day_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching 3 days prior and post data for the latest outlier for vailidating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an outlier identifier starting from 1 or any specific number\n",
    "outlier_id = 1\n",
    "\n",
    "# Convert start_date, end_date, and outlier_date to Timestamp for consistent comparison\n",
    "start_date_co = pd.Timestamp(date_ranges['start_date'].iloc[-1])\n",
    "end_date_co = pd.Timestamp(date_ranges['end_date'].iloc[-1]) + pd.Timedelta(days=1)  # Extend the end date by one additional day\n",
    "outlier_date_co = pd.Timestamp(date_ranges['outlier_date'].iloc[-1])\n",
    "\n",
    "# Get hourly data for the range including 3 days before and after the outlier\n",
    "hourly_data = fetch_and_process_hourly_data(symbol, start_date_co, end_date_co, api_key)\n",
    "\n",
    "# Assign the current outlier_id to the data\n",
    "hourly_data['outlier_id'] = outlier_id\n",
    "\n",
    "# Filter out weekdends\n",
    "hourly_data = hourly_data[~hourly_data['date'].dt.weekday.isin([5,6])]\n",
    "\n",
    "# prior_data from start_date to outlier_date inclusive\n",
    "df_prior = hourly_data[(hourly_data['date'] >= start_date_co) & (hourly_data['date'] < outlier_date_co)]\n",
    "df_prior[\"day type\"] = \"prior day\"\n",
    "\n",
    "# outlier_data is for the hourly data on the day of the outlier\n",
    "df_outlier = hourly_data[(hourly_data['date'].dt.date == outlier_date_co.date())]\n",
    "df_outlier[\"day type\"] = \"outlier day\"\n",
    "\n",
    "# post_data from the day after outlier_date to end_date\n",
    "post_outlier_co = outlier_date_co + pd.Timedelta(days=1)  # Starting the day after the outlier_date\n",
    "df_post = hourly_data[(hourly_data['date'] >= post_outlier_co) & (hourly_data['date'] < end_date_co)]\n",
    "df_post[\"day type\"] = \"post day\"\n",
    "\n",
    "new_df = pd.concat([df_prior, df_outlier, df_post], axis=0)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to prepare sequences for training\n",
    "# def prepare_sequences(df):\n",
    "#     sequences = []\n",
    "#     labels = []\n",
    "    \n",
    "#     unique_ids = df['outlier_id'].unique()\n",
    "    \n",
    "#     for oid in unique_ids:\n",
    "#         prior_data = df[(df['outlier_id'] == oid) & (df['day_type'].isin(['prior day', 'outlier day']))]['c'].values\n",
    "#         post_data = df[(df['outlier_id'] == oid) & (df['day_type'] == 'post day')]['c'].values\n",
    "        \n",
    "#         if len(prior_data) >= 72 and len(post_data) >= 72:  # Ensure full sequences\n",
    "#             sequences.append(prior_data[-72:])\n",
    "#             labels.append(post_data[:72])  # Only the first 3 post days are needed\n",
    "            \n",
    "#     return np.array(sequences), np.array(labels)\n",
    "\n",
    "# def train_model(df_train):\n",
    "# \tdf_train.fillna(method='bfill', inplace=True)\n",
    "# \tscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# \tdf_train['c'] = scaler.fit_transform(df_train[['c']])\n",
    "\n",
    "# \tX_train, y_train = prepare_sequences(df_train)\n",
    "# \tif X_train.size == 0:\n",
    "# \t\treturn None, None  # Early exit if no training data\n",
    "\n",
    "# \tX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# \tmodel = Sequential([\n",
    "# \t\tLSTM(50, return_sequences=True, input_shape=(72, 1)),\n",
    "# \t\tLSTM(50),\n",
    "# \t\tDense(72)\n",
    "# \t])\n",
    "# \tmodel.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "# \tmodel.fit(X_train, y_train, epochs=100, batch_size=64, verbose=2)\n",
    "# \treturn model, scaler\n",
    "\n",
    "# # Function to prepare a single sequence for prediction\n",
    "# def prepare_sequence_predict(df_prior, scaler):\n",
    "# \tdf_prior.drop(columns= ['prev_close', 'returns', 'n'], inplace=True)\n",
    "# \tdf_prior.fillna(method='bfill', inplace=True)\n",
    "# \tprior_data = df_prior['c'].values # \"df_prior\" should be the dataframe containing prior day's data for the current outlier.\n",
    "# \tprior_data = scaler.transform(prior_data.reshape(-1, 1)).flatten()\n",
    "# \treturn prior_data.reshape(1, 72, 1)\n",
    "\n",
    "# # Function to predict using the model\n",
    "# def predict(model, df_prior, scaler):\n",
    "#     X_test = prepare_sequence_predict(df_prior, scaler)\n",
    "#     predictions = model.predict(X_test)\n",
    "#     return scaler.inverse_transform(predictions.reshape(-1, 1))\n",
    "\n",
    "# # Function to validate predictions\n",
    "# def validate_predictions(predictions, df_post, scaler):\n",
    "# \t#df_post.drop(columns= ['prev_close', 'returns', 'n'], inplace=True)\n",
    "# \tdf_post.fillna(method='ffill', inplace=True)\n",
    "# \tpost_data = df_post['c'].values[:72] # \"df_post\" should be the dataframe containing post day's data for the current outlier.\n",
    "# \tpost_data_scaled = scaler.transform(post_data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# \tmse = mean_squared_error(post_data_scaled, predictions.flatten())\n",
    "# \tmae = mean_absolute_error(post_data_scaled, predictions.flatten())\n",
    "# \trmse = np.sqrt(mse)\n",
    "# \tr2 = r2_score(post_data_scaled, predictions.flatten())\n",
    "\t\n",
    "# \treturn mse, mae, rmse, r2\n",
    "\n",
    "# # Example usage:\n",
    "# model, scaler = train_model(df_train)\n",
    "# predictions = predict(model, df_prior, scaler)\n",
    "# mse, mae, rmse, r2 = validate_predictions(predictions, df_post, scaler)\n",
    "\n",
    "# # Now you can print these values in a different cell\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"Mean Absolute Error: {mae}\")\n",
    "# print(f\"Root Mean Squared Error: {rmse}\")\n",
    "# print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error: 15.63521443905872\n",
    "\n",
    "Mean Absolute Error: 3.953213237589056\n",
    "\n",
    "Root Mean Squared Error: 3.954138899818609\n",
    "\n",
    "R-squared: -2287.3112314300106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = new_df.sort_values(by='date', ascending=True)\n",
    "sorted_df.fillna(method='bfill', inplace=True)\n",
    "sorted_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_set = sorted_df.iloc[:264].reset_index(drop=True)\n",
    "test_set = sorted_df.iloc[264:].reset_index(drop=True)\n",
    "\n",
    "# Reset index if needed\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 240  # Example sequence length for LSTM model - 10 days - 240 hours\n",
    "\n",
    "# Filter data for the single outlier_id\n",
    "outlier_id = train_set[\"outlier_id\"].unique()[0]\n",
    "train_df = train_set[train_set[\"outlier_id\"] == outlier_id].set_index(\"date\")\n",
    "train_df.index = pd.to_datetime(train_df.index)  # Convert index to DateTimeIndex\n",
    "test_df = test_set[test_set[\"outlier_id\"] == outlier_id]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[[\"c\"]])\n",
    "\n",
    "# Prepare data for LSTM model\n",
    "train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "# Define and compile LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "# Prepare last sequence for forecasting\n",
    "last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "# Iteratively forecast the next 240 steps\n",
    "forecast_steps = 240\n",
    "predictions_scaled = []\n",
    "for _ in range(forecast_steps):\n",
    "    # Reshape the last sequence for prediction\n",
    "    last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "    # Predict the next step and append to predictions\n",
    "    next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "    predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "    # Update the last sequence with the prediction\n",
    "    last_sequence = np.roll(last_sequence, -1)\n",
    "    last_sequence[-1] = next_step_pred\n",
    "\n",
    "# Inverse transform predictions\n",
    "predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "# Compute metrics for the single outlier_id\n",
    "actuals = test_df[\"c\"].values[:forecast_steps]\n",
    "\n",
    "mse = mean_squared_error(actuals, predictions_inv)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actuals, predictions_inv)\n",
    "mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "r2 = r2_score(actuals, predictions_inv)\n",
    "\n",
    "# Calculate sMAPE\n",
    "smape = np.mean(2 * np.abs(predictions_inv - actuals) / (np.abs(predictions_inv) + np.abs(actuals))) * 100\n",
    "\n",
    "# Print metrics\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAPE:\", mape)\n",
    "print(\"sMAPE:\", smape)\n",
    "print(\"R^2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(predictions_inv)\n",
    "\n",
    "prediction.to_csv('3.csv')\n",
    "sorted_df.to_csv('4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .iloc(-2)\n",
    "1. MAE: 0.01127558805931891\n",
    "2. MSE: 0.00015902518543765831\n",
    "3. RMSE: 0.012610518840938239\n",
    "4. MAPE: 0.013025547450473286\n",
    "5. sMAPE: 1.3092876639982067\n",
    "6. R^2: -3.1421988118014763\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 step prediction - 3 hours\n",
    "1. RMSE: 0.0014249278565365072\n",
    "2. MAE: 0.0013819148000081178\n",
    "3. MAPE: 0.0016188677009796257\n",
    "4. sMAPE: 0.16202621303027964\n",
    "5. R^2: -152.04668816591115\n",
    "\n",
    "#### 24 steps predictions - 24 hours - 1 day\n",
    "1. RMSE: 0.0034528471986093857\n",
    "2. MAE: 0.0031602989514668786\n",
    "3. MAPE: 0.0037021375133191553\n",
    "4. sMAPE: 0.38530215208360313\n",
    "5. R^2: -3.3388885069594343\n",
    "\n",
    "#### 216 steps - 9 days\n",
    "1. MAE: 0.003084491074504933\n",
    "2. MSE: 1.2475740906942047e-05\n",
    "3. RMSE: 0.0035321014859346902\n",
    "4. MAPE: 0.0036341938240416825\n",
    "5. sMAPE: 0.4224267720020953\n",
    "6. R^2: 0.11754503942796934\n",
    "\n",
    "#### 240 steps - 10 days\n",
    "1. MAE: 0.002961868462021624\n",
    "2. MSE: 1.2733241896113776e-05\n",
    "3. RMSE: 0.003568366838781262\n",
    "4. MAPE: 0.0034965434280448044\n",
    "5. sMAPE: 0.36766986776281707\n",
    "6. R^2: 0.07253019544913986\n",
    "\n",
    "#### 336 steps - 14 days\n",
    "1. MAE: 0.009938953848609888\n",
    "2. MSE: 0.00013717247923436177\n",
    "3. RMSE: 0.011712065540901049\n",
    "4. MAPE: 0.011490952565699\n",
    "5. PE: 1.1586982793510603\n",
    "6. R^2: -2.244672531975713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Params for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time series forecasting, the following evaluation metrics are commonly used to assess model performance:\n",
    "\n",
    "1. **Mean Absolute Error (MAE):**\n",
    "   - Measures the average magnitude of the errors between predicted and actual values, regardless of direction.\n",
    "   - Formula: \n",
    "\t \n",
    "     $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|$\n",
    "\n",
    "   - **Pros:** Easy to interpret.\n",
    "   - **Cons:** Treats all errors equally, without penalizing larger errors more.\n",
    "\n",
    "\n",
    "2. **Mean Squared Error (MSE):**\n",
    "   - Measures the average of the squared errors, which penalizes larger errors more heavily.\n",
    "   - Formula: \n",
    "\n",
    "     $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2$\n",
    "\n",
    "   - **Pros:** Highlights larger errors due to squaring, which can be useful if large deviations are particularly undesirable.\n",
    "   - **Cons:** Can be sensitive to outliers.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):**\n",
    "   - The square root of MSE, bringing the error metric back to the original scale of the data.\n",
    "   - Formula: \n",
    "\n",
    "     $\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}$\n",
    "\n",
    "   - **Pros:** More interpretable than MSE due to being on the same scale as the data.\n",
    "   - **Cons:** Still sensitive to outliers.\n",
    "\n",
    "4. **Mean Absolute Percentage Error (MAPE):**\n",
    "   - Measures the average percentage error between the forecasted and actual values.\n",
    "   - Formula: \n",
    "\n",
    "     $\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$\n",
    "\n",
    "   - **Pros:** Expresses errors as a percentage, which can be easier to interpret across different datasets.\n",
    "   - **Cons:** Can be misleading if actual values are close to zero, leading to high error rates.\n",
    "\n",
    "5. **Symmetric Mean Absolute Percentage Error (sMAPE):**\n",
    "   - A variation of MAPE that addresses its sensitivity to extreme values.\n",
    "   - Formula: \n",
    "\n",
    "     $\\text{sMAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\frac{\\left| y_i - \\hat{y}_i \\right|}{\\left( |y_i| + |\\hat{y}_i| \\right) / 2}$\n",
    "\n",
    "   - **Pros:** Less sensitive to large percentage errors.\n",
    "   - **Cons:** More complex to interpret.\n",
    "\n",
    "6. **R-squared (Coefficient of Determination):**\n",
    "   - Measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "   - Formula:\n",
    "\n",
    "     $R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "\n",
    "   - **Pros:** Indicates the goodness-of-fit.\n",
    "   - **Cons:** Not always informative for time series data, as it assumes independence between observations.\n",
    "\n",
    "7. **Mean Absolute Scaled Error (MASE):**\n",
    "   - Compares the forecasting model’s performance to a naïve forecast (e.g., last observed value).\n",
    "   - Formula: \n",
    "\n",
    "     $\\text{MASE} = \\frac{\\text{MAE}}{\\text{MAE of naïve forecast}}$\n",
    "\n",
    "   - **Pros:** Useful for comparing models across different datasets.\n",
    "   - **Cons:** Requires a good benchmark model for comparison.\n",
    "\n",
    "In our case of predicting close prices, MAE, RMSE, and MASE are typically the most relevant because they directly measure the error magnitude in the same units as the data, making them more interpretable for financial time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Params for Financial Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the standard metrics, the following metrics can provide deeper insights into the performance of our time series forecasting model in the forex market:\n",
    "\n",
    "1. **Directional Accuracy (DA):**\n",
    "   - Measures how often the model correctly predicts the direction of price movement (up or down).\n",
    "   - Formula:\n",
    "\n",
    "     $\\text{DA} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}\\left[\\text{sign}(\\hat{y}_i - y_{i-1}) = \\text{sign}(y_i - y_{i-1})\\right]$\n",
    "\n",
    "   - **Pros:** Directly relevant for trading strategies, as correct direction prediction is often more critical than exact value prediction.\n",
    "   - **Cons:** Does not account for the magnitude of the forecast error.\n",
    "\n",
    "2. **Profit and Loss (P&L):**\n",
    "   - Evaluates the hypothetical profit or loss if a trading strategy based on the model's predictions was implemented.\n",
    "   - **Pros:** Provides practical insights into how well the model would perform in real trading scenarios.\n",
    "   - **Cons:** Requires a well-defined trading strategy to simulate P&L.\n",
    "\n",
    "3. **Hit Rate (HR):**\n",
    "   - Measures the percentage of times the model's predicted direction and the actual market direction match.\n",
    "   - **Pros:** Useful for understanding how often the model makes correct directional calls.\n",
    "   - **Cons:** Does not quantify the magnitude of errors when the direction is incorrect.\n",
    "\n",
    "4. **Maximum Drawdown (MDD):**\n",
    "   - Measures the largest peak-to-trough decline in a simulated trading equity curve using the model's predictions.\n",
    "   - **Pros:** Helps assess the risk of the model in terms of potential losses.\n",
    "   - **Cons:** Requires simulation over a trading strategy to calculate.\n",
    "\n",
    "5. **Sharpe Ratio:**\n",
    "   - Evaluates the risk-adjusted return of a trading strategy based on the model's predictions.\n",
    "   - Formula:\n",
    "\n",
    "     \\text{Sharpe Ratio} = \\frac{\\text{Average P\\&L} - \\text{Risk-Free Rate}}{\\text{Standard Deviation of P\\&L}}\n",
    "\n",
    "   - **Pros:** Balances returns with the associated risk, offering a comprehensive measure of strategy performance.\n",
    "   - **Cons:** Assumes returns are normally distributed, which might not always hold in financial markets.\n",
    "\n",
    "6. **Mean Directional Accuracy (MDA):**\n",
    "   - Similar to directional accuracy but gives a more refined measure by focusing on the average accuracy of the directional predictions.\n",
    "   - Formula:\n",
    "\n",
    "     \\text{MDA} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\mathbb{1}\\left[\\text{sign}(\\hat{y}_i - y_{i-1}) = \\text{sign}(y_i - y_{i-1})\\right]}{n}\n",
    "\n",
    "   - **Pros:** Useful when focusing on directionality rather than magnitude.\n",
    "   - **Cons:** Overlooks large errors in magnitude.\n",
    "\n",
    "7. **Volatility Adjusted Return (VAR):**\n",
    "   - Measures return adjusted for volatility to understand how the model performs under varying market conditions.\n",
    "   - **Pros:** Important in markets like forex, where volatility can significantly impact performance.\n",
    "   - **Cons:** Requires more complex calculation and an understanding of market volatility.\n",
    "\n",
    "8. **Value at Risk (VaR):**\n",
    "   - Estimates the potential loss in value of a portfolio over a defined period for a given confidence interval.\n",
    "   - **Pros:** Helps assess the potential downside risk.\n",
    "   - **Cons:** Does not account for losses beyond the confidence interval threshold.\n",
    "\n",
    "Using these metrics in combination with the standard error metrics can give us a more comprehensive understanding of how our model might perform in real-world trading scenarios, particularly in the highly volatile forex market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
