{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pandas.tseries.offsets import BDay\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_return(df):\n",
    "    # Use pct_change() to calculate the percentage change in 'c' (close prices)\n",
    "    df['daily_return'] = df['c'].pct_change()\n",
    "    df['abs_daily_return'] = df['daily_return'].abs()\n",
    "    return df\n",
    "\n",
    "def get_top_outliers(df, n=10):\n",
    "    # # Filter for positive returns with 'c' > 0.95 and negative returns with 'c' < 0.8\n",
    "    # positive_condition = (df['c'] > 0.95) & (df['daily_return'] > 0)\n",
    "    # negative_condition = (df['c'] < 0.8) & (df['daily_return'] < 0)\n",
    "    \n",
    "    # # Combine both conditions to create the final filter\n",
    "    # filtered_df = df[positive_condition | negative_condition]\n",
    "    \n",
    "    # # Get the top n rows with the largest absolute daily returns\n",
    "    # return filtered_df.nlargest(n, 'abs_daily_return')\n",
    "    return df.nlargest(n, 'abs_daily_return')\n",
    "\n",
    "# Define the API key and base URL\n",
    "api_key = 'beBybSi8daPgsTp5yx5cHtHpYcrjp5Jq'\n",
    "\n",
    "# Define the currency pairs and years\n",
    "pair = \"C:USDEUR\"\n",
    "years = range(2022, 2024)\n",
    "\n",
    "# Initialize DataFrames\n",
    "full_data = pd.DataFrame()\n",
    "outliers_data = pd.DataFrame()\n",
    "\n",
    "# Loop over each year\n",
    "for year in years:\n",
    "    # Format the API endpoint\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{pair}/range/1/day/{start_date}/{end_date}?adjusted=true&sort=asc&limit=50000&apiKey={api_key}\"\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200 and 'results' in data:\n",
    "        # Load data into a DataFrame\n",
    "        df = pd.DataFrame(data['results'])\n",
    "        # Convert timestamps\n",
    "        df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "        df.drop(columns=['t'], inplace=True)\n",
    "\n",
    "        df = df[df['date'].dt.weekday < 5]\n",
    "        # Calculating returns\n",
    "        df = calculate_daily_return(df)\n",
    "\n",
    "        # Append the data to the full_data DataFrame for the current currency pair\n",
    "        df['year'] = year\n",
    "        df['day'] = df['date'].dt.day_name()\n",
    "        # Find the top 10 outliers based on absolute values of the daily return value\n",
    "        # df['abs_daily_return'] = df['c'].abs()\n",
    "        top_outliers = df.nlargest(10, 'abs_daily_return')\n",
    "        outlier_dates = top_outliers['date']\n",
    "\n",
    "        # Create a new column 'is_outlier' in the full_data DataFrame\n",
    "        df['is_outlier'] = df['date'].isin(outlier_dates).astype(int)\n",
    "        full_data = pd.concat([full_data, df], ignore_index=True)\n",
    "        \n",
    "        # Append outliers to the outliers_data DataFrame for the current currency pair\n",
    "        top_outliers['year'] = year\n",
    "        outliers_data = pd.concat([outliers_data, top_outliers], ignore_index=True)\n",
    "        \n",
    "\n",
    "sorted_full_data = full_data.sort_values(by=\"date\")\n",
    "sorted_outliers_data = outliers_data.sort_values(by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "      <th>vw</th>\n",
       "      <th>o</th>\n",
       "      <th>c</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>n</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>abs_daily_return</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>is_outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57901</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.87925</td>\n",
       "      <td>0.88478</td>\n",
       "      <td>0.88651</td>\n",
       "      <td>0.879020</td>\n",
       "      <td>57901</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62051</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>0.88460</td>\n",
       "      <td>0.88624</td>\n",
       "      <td>0.88709</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>62051</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>2022</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64819</td>\n",
       "      <td>0.8838</td>\n",
       "      <td>0.88627</td>\n",
       "      <td>0.88390</td>\n",
       "      <td>0.88673</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>64819</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>-0.002640</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>2022</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68551</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>0.88409</td>\n",
       "      <td>0.88520</td>\n",
       "      <td>0.88613</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>68551</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>2022</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45968</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.88531</td>\n",
       "      <td>0.88000</td>\n",
       "      <td>0.88571</td>\n",
       "      <td>0.879662</td>\n",
       "      <td>45968</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>-0.005874</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>2022</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>6522</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>0.90750</td>\n",
       "      <td>0.90747</td>\n",
       "      <td>0.90940</td>\n",
       "      <td>0.901600</td>\n",
       "      <td>6522</td>\n",
       "      <td>2023-12-25</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>2023</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>51232</td>\n",
       "      <td>0.9068</td>\n",
       "      <td>0.90747</td>\n",
       "      <td>0.90550</td>\n",
       "      <td>0.90833</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>51232</td>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>-0.002171</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>2023</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>77389</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.90557</td>\n",
       "      <td>0.90018</td>\n",
       "      <td>0.90670</td>\n",
       "      <td>0.898796</td>\n",
       "      <td>77389</td>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>-0.005875</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>2023</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>83788</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>0.90018</td>\n",
       "      <td>0.90348</td>\n",
       "      <td>0.90453</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>83788</td>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>2023</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>78410</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.90346</td>\n",
       "      <td>0.90580</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>78410</td>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>2023</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v      vw        o        c        h         l      n       date  \\\n",
       "0    57901  0.8828  0.87925  0.88478  0.88651  0.879020  57901 2022-01-03   \n",
       "1    62051  0.8854  0.88460  0.88624  0.88709  0.883000  62051 2022-01-04   \n",
       "2    64819  0.8838  0.88627  0.88390  0.88673  0.881100  64819 2022-01-05   \n",
       "3    68551  0.8844  0.88409  0.88520  0.88613  0.882300  68551 2022-01-06   \n",
       "4    45968  0.8833  0.88531  0.88000  0.88571  0.879662  45968 2022-01-07   \n",
       "..     ...     ...      ...      ...      ...       ...    ...        ...   \n",
       "515   6522  0.9078  0.90750  0.90747  0.90940  0.901600   6522 2023-12-25   \n",
       "516  51232  0.9068  0.90747  0.90550  0.90833  0.905100  51232 2023-12-26   \n",
       "517  77389  0.9029  0.90557  0.90018  0.90670  0.898796  77389 2023-12-27   \n",
       "518  83788  0.9008  0.90018  0.90348  0.90453  0.897500  83788 2023-12-28   \n",
       "519  78410  0.9040  0.90346  0.90580  0.90625  0.902000  78410 2023-12-29   \n",
       "\n",
       "     daily_return  abs_daily_return  year        day  is_outlier  \n",
       "0             NaN               NaN  2022     Monday           0  \n",
       "1        0.001650          0.001650  2022    Tuesday           0  \n",
       "2       -0.002640          0.002640  2022  Wednesday           0  \n",
       "3        0.001471          0.001471  2022   Thursday           0  \n",
       "4       -0.005874          0.005874  2022     Friday           0  \n",
       "..            ...               ...   ...        ...         ...  \n",
       "515      0.000629          0.000629  2023     Monday           0  \n",
       "516     -0.002171          0.002171  2023    Tuesday           0  \n",
       "517     -0.005875          0.005875  2023  Wednesday           0  \n",
       "518      0.003666          0.003666  2023   Thursday           0  \n",
       "519      0.002568          0.002568  2023     Friday           0  \n",
       "\n",
       "[520 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>outlier_date</th>\n",
       "      <th>outlier_price</th>\n",
       "      <th>daily_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-17</td>\n",
       "      <td>2022-03-30</td>\n",
       "      <td>2022-03-09</td>\n",
       "      <td>0.90302</td>\n",
       "      <td>-0.015868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-15</td>\n",
       "      <td>2022-07-26</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>0.97493</td>\n",
       "      <td>0.017163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>2022-07-11</td>\n",
       "      <td>0.99537</td>\n",
       "      <td>0.014235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-24</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>1.00213</td>\n",
       "      <td>0.015134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>2022-09-21</td>\n",
       "      <td>1.01711</td>\n",
       "      <td>0.014371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-05</td>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>1.03190</td>\n",
       "      <td>0.015290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-14</td>\n",
       "      <td>2022-10-25</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>1.00150</td>\n",
       "      <td>-0.014950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>2022-11-25</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>1.00150</td>\n",
       "      <td>-0.023575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>0.98115</td>\n",
       "      <td>-0.016913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>2022-12-02</td>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>0.96400</td>\n",
       "      <td>-0.017479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>2023-01-27</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>0.93760</td>\n",
       "      <td>-0.013468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-01-12</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>0.90790</td>\n",
       "      <td>-0.013935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>0.94780</td>\n",
       "      <td>0.012931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-02-23</td>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>0.94499</td>\n",
       "      <td>0.013720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>0.92990</td>\n",
       "      <td>-0.013002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>0.91533</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.91164</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>0.89190</td>\n",
       "      <td>-0.016833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-04-18</td>\n",
       "      <td>2023-05-29</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>0.90920</td>\n",
       "      <td>0.019397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>0.91925</td>\n",
       "      <td>-0.016235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_date   end_date outlier_date  outlier_price  daily_return\n",
       "0  2022-02-17 2022-03-30   2022-03-09        0.90302     -0.015868\n",
       "1  2022-06-15 2022-07-26   2022-07-05        0.97493      0.017163\n",
       "2  2022-06-21 2022-08-01   2022-07-11        0.99537      0.014235\n",
       "3  2022-08-24 2022-10-04   2022-09-13        1.00213      0.015134\n",
       "4  2022-09-01 2022-10-12   2022-09-21        1.01711      0.014371\n",
       "5  2022-09-05 2022-10-14   2022-09-23        1.03190      0.015290\n",
       "6  2022-09-14 2022-10-25   2022-10-04        1.00150     -0.014950\n",
       "7  2022-10-17 2022-11-25   2022-11-04        1.00150     -0.023575\n",
       "8  2022-10-21 2022-12-01   2022-11-10        0.98115     -0.016913\n",
       "9  2022-10-24 2022-12-02   2022-11-11        0.96400     -0.017479\n",
       "10 2022-12-19 2023-01-27   2023-01-06        0.93760     -0.013468\n",
       "11 2023-01-12 2023-02-22   2023-02-01        0.90790     -0.013935\n",
       "12 2023-02-15 2023-03-28   2023-03-07        0.94780      0.012931\n",
       "13 2023-02-23 2023-04-05   2023-03-15        0.94499      0.013720\n",
       "14 2023-02-27 2023-04-07   2023-03-17        0.92990     -0.013002\n",
       "15 2023-03-28 2023-05-08   2023-04-17        0.91533      0.016469\n",
       "16 2023-04-11 2023-05-22   2023-05-01        0.91164      0.014511\n",
       "17 2023-04-17 2023-05-26   2023-05-05        0.89190     -0.016833\n",
       "18 2023-04-18 2023-05-29   2023-05-08        0.90920      0.019397\n",
       "19 2023-10-25 2023-12-05   2023-11-14        0.91925     -0.016235"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dates in dataset to datetime objects\n",
    "sorted_outliers_data['date'] = pd.to_datetime(sorted_outliers_data['date'])\n",
    "\n",
    "date_ranges = pd.DataFrame({\n",
    "    \"start_date\": sorted_outliers_data['date'] - BDay(14), # To predict X days, keep this as X-1 (as 1 day of outlier will be considered in LSTM input)\n",
    "    \"end_date\": sorted_outliers_data['date'] + BDay(15),\n",
    "    \"outlier_date\": sorted_outliers_data['date'],\n",
    "    \"outlier_price\": sorted_outliers_data['c'],\n",
    "    \"daily_return\": sorted_outliers_data['daily_return']\n",
    "})\n",
    "\n",
    "date_ranges.reset_index(drop=True, inplace=True)\n",
    "\n",
    "date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily_data(pair, start_date, end_date, api_key):\n",
    "    formatted_start_date = start_date.strftime('%Y-%m-%d')\n",
    "    formatted_end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{pair}/range/1/day/{formatted_start_date}/{formatted_end_date}?adjusted=true&sort=asc&apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "    response_data = response.json()\n",
    "\n",
    "    if 'results' not in response_data:\n",
    "        print(f\"No 'results' in response: {response_data}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(response_data['results'])\n",
    "    df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "    df.drop(columns=['t'], inplace=True)\n",
    "\n",
    "    daily_data = calculate_daily_return(df)\n",
    "    daily_data.set_index('date', inplace=True)\n",
    "\n",
    "    return daily_data\n",
    "\n",
    "def fetch_and_process_daily_data(pair, start_date, end_date, api_key):\n",
    "    daily_data = fetch_daily_data(pair, start_date, end_date, api_key)\n",
    "\n",
    "    if daily_data is None:\n",
    "        print(\"No data fetched\")\n",
    "        return None\n",
    "\n",
    "    daily_data.reset_index(inplace=True)\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='bfill', inplace=True)\n",
      "/var/folders/9n/nyfs9h7n2lsfs0vd2lq0589h0000gn/T/ipykernel_7339/1944401145.py:32: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  daily_data.fillna(method='ffill', inplace=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the model performance metrics for each outlier\n",
    "trade_results_list = []\n",
    "\n",
    "# Define the profit threshold for entry (0.01% profit target)\n",
    "profit_threshold = 0.1  # 0.1% in decimal form\n",
    "\n",
    "# Loop through each row in the date_ranges DataFrame\n",
    "for idx, row in date_ranges.iterrows():\n",
    "    outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "    start_date_co = pd.Timestamp(row['start_date'])\n",
    "    end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "    outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "    # Fetch and process daily data for the current range\n",
    "    daily_data = fetch_and_process_daily_data(pair, start_date_co, end_date_co, api_key)\n",
    "\n",
    "    if daily_data is None:\n",
    "        print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "        continue  # Skip to the next iteration if no data\n",
    "\n",
    "    # Assign the current outlier_id to the data\n",
    "    daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "    # Filter out weekends\n",
    "    daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "    # Sort data by date\n",
    "    daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "    # Fill missing values\n",
    "    daily_data.fillna(method='bfill', inplace=True)\n",
    "    daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "    test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "    # Normalize the data using only the training data\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "    # Prepare data for LSTM model\n",
    "    sequence_length = 12\n",
    "    train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "    # Define and compile LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "    # Prepare the last sequence for forecasting\n",
    "    last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "    # Forecast the next steps\n",
    "    forecast_steps = len(test_set)\n",
    "    predictions_scaled = []\n",
    "    for _ in range(forecast_steps):\n",
    "        last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "        next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "        predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "        last_sequence = np.roll(last_sequence, -1)\n",
    "        last_sequence[-1] = next_step_pred\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "    # Actual values for comparison\n",
    "    actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    mse = mean_squared_error(actuals, predictions_inv)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "    accuracy = np.round(100 - (mape * 100), 2)\n",
    "\n",
    "    # Determine if the outlier is positive or negative\n",
    "    outlier_return = row['daily_return']\n",
    "    is_positive_outlier = outlier_return > 0  # Check if the outlier is positive\n",
    "    outlier_type = \"Positive\" if is_positive_outlier else \"Negative\"  # Define outlier type\n",
    "\n",
    "    # Trade logic based on outlier type\n",
    "    entry_price = None\n",
    "    exit_price = None\n",
    "    entry_date = None\n",
    "    exit_date = None\n",
    "    profit = 0\n",
    "    days_held = 0\n",
    "    trade_initiated = False\n",
    "    position_type = \"No Trade\"  # Default if no trade is initiated\n",
    "\n",
    "    for i in range(1, len(predictions_inv)):\n",
    "        predicted_change = (predictions_inv[i] - predictions_inv[i - 1]) * 100 / predictions_inv[i - 1]\n",
    "        actual_change = (actuals[i] - actuals[i - 1]) * 100 / actuals[i - 1]\n",
    "\n",
    "        if is_positive_outlier:\n",
    "            # Positive Outlier Logic (Long Position)\n",
    "            # Trend continuation: Enter long position if predicted trend continues\n",
    "            if entry_price is None and predicted_change >= profit_threshold:\n",
    "                future_predictions = predictions_inv[i:i + 2]  # Check the next 2 predicted values\n",
    "                if all((future_predictions[j] - future_predictions[j - 1]) * 100 / future_predictions[j - 1] >= profit_threshold for j in range(1, len(future_predictions))):\n",
    "                    entry_price = actuals[i - 1]\n",
    "                    entry_date = test_set['date'].iloc[i - 1]\n",
    "                    trade_initiated = True\n",
    "                    position_type = \"Long\"\n",
    "            # Trend reversal: Exit long position if predicted change turns negative\n",
    "            elif entry_price is not None and predicted_change < 0:\n",
    "                exit_price = actuals[i - 1]\n",
    "                exit_date = test_set['date'].iloc[i - 1]\n",
    "                profit = (exit_price - entry_price) * 100 / entry_price\n",
    "                days_held = (exit_date - entry_date).days\n",
    "                break\n",
    "        else:\n",
    "            # Negative Outlier Logic (Short Position)\n",
    "            # Trend continuation: Short the market if predicted trend continues\n",
    "            if entry_price is None and predicted_change <= -profit_threshold:\n",
    "                future_predictions = predictions_inv[i:i + 2]  # Check the next 2 predicted values\n",
    "                if all((future_predictions[j] - future_predictions[j - 1]) * 100 / future_predictions[j - 1] <= -profit_threshold for j in range(1, len(future_predictions))):\n",
    "                    entry_price = actuals[i - 1]\n",
    "                    entry_date = test_set['date'].iloc[i - 1]\n",
    "                    trade_initiated = True\n",
    "                    position_type = \"Short\"\n",
    "            # Trend reversal: Exit short position if predicted change turns positive\n",
    "            elif entry_price is not None and predicted_change > 0:\n",
    "                exit_price = actuals[i - 1]\n",
    "                exit_date = test_set['date'].iloc[i - 1]\n",
    "                profit = (entry_price - exit_price) * 100 / entry_price  # Short trade profit formula\n",
    "                days_held = (exit_date - entry_date).days\n",
    "                break\n",
    "\n",
    "    # If no reversal is detected, exit at the last available date\n",
    "    if trade_initiated and entry_price is not None and exit_price is None:\n",
    "        exit_price = actuals[-1]\n",
    "        exit_date = test_set['date'].iloc[-1]\n",
    "        if is_positive_outlier:\n",
    "            profit = (exit_price - entry_price) * 100 / entry_price  # Long trade profit formula\n",
    "        else:\n",
    "            profit = (entry_price - exit_price) * 100 / entry_price  # Short trade profit formula\n",
    "        days_held = (exit_date - entry_date).days\n",
    "\n",
    "    # Adjust \"Position Type\" based on whether a trade was initiated or not\n",
    "    if not trade_initiated:\n",
    "        if is_positive_outlier:\n",
    "            position_type = \"Go Short (Sell!)\"\n",
    "        else:\n",
    "            position_type = \"Go Long (Buy!)\"\n",
    "\n",
    "    # Append trade results for every outlier, even if no trade was initiated\n",
    "    if profit >= 0 and rmse < 0.5:\n",
    "        trade_results_list.append({\n",
    "\t\t\t'Outlier Date': outlier_date_co,\n",
    "\t\t\t'Outlier Price': row['outlier_price'],\n",
    "\t\t\t'Outlier Type': outlier_type,  # New column for outlier type\n",
    "\t\t\t'Position Type': position_type,  # Adjusted position type\n",
    "\t\t\t'Entry Date': entry_date,\n",
    "\t\t\t'Entry Price': entry_price,\n",
    "\t\t\t'Exit Date': exit_date,\n",
    "\t\t\t'Exit Price': exit_price,\n",
    "\t\t\t'Days Held': days_held if trade_initiated else 0,\n",
    "\t\t\t'Profit': np.round(profit, 2) if trade_initiated else None,\n",
    "\t\t\t'model_RMSE': rmse,\n",
    "\t\t\t'model_accuracy': accuracy\n",
    "\t\t})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "trade_results_df = pd.DataFrame(trade_results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlier Date</th>\n",
       "      <th>Outlier Price</th>\n",
       "      <th>Outlier Type</th>\n",
       "      <th>Position Type</th>\n",
       "      <th>Entry Date</th>\n",
       "      <th>Entry Price</th>\n",
       "      <th>Exit Date</th>\n",
       "      <th>Exit Price</th>\n",
       "      <th>Days Held</th>\n",
       "      <th>Profit</th>\n",
       "      <th>model_RMSE</th>\n",
       "      <th>model_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-21</td>\n",
       "      <td>1.01711</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Go Short (Sell!)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>98.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>1.00150</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Short</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>0.96545</td>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>0.96513</td>\n",
       "      <td>2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.045054</td>\n",
       "      <td>95.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>0.98115</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Short</td>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>0.96400</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>0.96200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.017652</td>\n",
       "      <td>98.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>0.96400</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Short</td>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>0.96898</td>\n",
       "      <td>2022-12-02</td>\n",
       "      <td>0.94890</td>\n",
       "      <td>18</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.067754</td>\n",
       "      <td>94.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>0.93760</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Go Long (Buy!)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021365</td>\n",
       "      <td>97.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>0.90790</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Go Long (Buy!)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>98.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>0.94780</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Long</td>\n",
       "      <td>2023-03-13</td>\n",
       "      <td>0.93254</td>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>0.94499</td>\n",
       "      <td>2</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.013603</td>\n",
       "      <td>98.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>0.94499</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Long</td>\n",
       "      <td>2023-03-22</td>\n",
       "      <td>0.92001</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>0.92257</td>\n",
       "      <td>6</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.021360</td>\n",
       "      <td>97.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>0.92990</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Short</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>0.92750</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>0.92257</td>\n",
       "      <td>4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>98.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>0.91164</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Long</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>0.92499</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>0.92840</td>\n",
       "      <td>1</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.016727</td>\n",
       "      <td>98.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-05-05</td>\n",
       "      <td>0.89190</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Go Long (Buy!)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023184</td>\n",
       "      <td>97.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>0.91925</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Go Long (Buy!)</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008113</td>\n",
       "      <td>99.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outlier Date  Outlier Price Outlier Type     Position Type Entry Date  \\\n",
       "0    2022-09-21        1.01711     Positive  Go Short (Sell!)        NaT   \n",
       "1    2022-11-04        1.00150     Negative             Short 2022-11-15   \n",
       "2    2022-11-10        0.98115     Negative             Short 2022-11-11   \n",
       "3    2022-11-11        0.96400     Negative             Short 2022-11-14   \n",
       "4    2023-01-06        0.93760     Negative    Go Long (Buy!)        NaT   \n",
       "5    2023-02-01        0.90790     Negative    Go Long (Buy!)        NaT   \n",
       "6    2023-03-07        0.94780     Positive              Long 2023-03-13   \n",
       "7    2023-03-15        0.94499     Positive              Long 2023-03-22   \n",
       "8    2023-03-17        0.92990     Negative             Short 2023-03-24   \n",
       "9    2023-05-01        0.91164     Positive              Long 2023-05-22   \n",
       "10   2023-05-05        0.89190     Negative    Go Long (Buy!)        NaT   \n",
       "11   2023-11-14        0.91925     Negative    Go Long (Buy!)        NaT   \n",
       "\n",
       "    Entry Price  Exit Date  Exit Price  Days Held  Profit  model_RMSE  \\\n",
       "0           NaN        NaT         NaN          0     NaN    0.016195   \n",
       "1       0.96545 2022-11-17     0.96513          2    0.03    0.045054   \n",
       "2       0.96400 2022-11-16     0.96200          5    0.21    0.017652   \n",
       "3       0.96898 2022-12-02     0.94890         18    2.07    0.067754   \n",
       "4           NaN        NaT         NaN          0     NaN    0.021365   \n",
       "5           NaN        NaT         NaN          0     NaN    0.019498   \n",
       "6       0.93254 2023-03-15     0.94499          2    1.34    0.013603   \n",
       "7       0.92001 2023-03-28     0.92257          6    0.28    0.021360   \n",
       "8       0.92750 2023-03-28     0.92257          4    0.53    0.012827   \n",
       "9       0.92499 2023-05-23     0.92840          1    0.37    0.016727   \n",
       "10          NaN        NaT         NaN          0     NaN    0.023184   \n",
       "11          NaN        NaT         NaN          0     NaN    0.008113   \n",
       "\n",
       "    model_accuracy  \n",
       "0            98.64  \n",
       "1            95.69  \n",
       "2            98.41  \n",
       "3            94.27  \n",
       "4            97.74  \n",
       "5            98.04  \n",
       "6            98.82  \n",
       "7            97.80  \n",
       "8            98.74  \n",
       "9            98.40  \n",
       "10           97.65  \n",
       "11           99.19  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the model performance metrics for each outlier\n",
    "# model_perf_list = []\n",
    "\n",
    "# # Loop through each row in the date_ranges DataFrame\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(symbol, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Assign the current outlier_id to the data\n",
    "#     daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     sequence_length = 12\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     mse = mean_squared_error(actuals, predictions_inv)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(actuals, predictions_inv)\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "\n",
    "#     # Append the performance metrics to the list\n",
    "#     model_perf_list.append({\n",
    "#         'outlier_id': outlier_id,  # Each range will have a unique outlier_id starting from 1\n",
    "#         'outlier_date': row['outlier_date'],\n",
    "#         'MAE': mae,\n",
    "#         'MSE': mse,\n",
    "#         'RMSE': rmse,\n",
    "#         'MAPE': mape\n",
    "#     })\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# model_perf = pd.DataFrame(model_perf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function to find entry and exit points based on moving averages\n",
    "# def find_entry_exit(predictions, window_short=5, window_long=10):\n",
    "    # \"\"\"Find entry and exit points based on the moving average of predictions.\"\"\"\n",
    "#     # Calculate short-term and long-term moving averages\n",
    "#     short_ma = predictions.rolling(window=window_short).mean()\n",
    "#     long_ma = predictions.rolling(window=window_long).mean()\n",
    "\n",
    "#     # Generate signals: 1 for buy, -1 for sell\n",
    "#     buy_signal = (short_ma > long_ma) & (short_ma.shift(1) <= long_ma.shift(1))\n",
    "#     sell_signal = (short_ma < long_ma) & (short_ma.shift(1) >= long_ma.shift(1))\n",
    "\n",
    "#     return buy_signal, sell_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA of wavelegth of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_wavelength(predictions, wavelet='gaus2', scales=np.logspace(0.1, 1.5, num=20, base=10)):\n",
    "#     \"\"\"Calculate the wavelength using Continuous Wavelet Transform (CWT).\"\"\"\n",
    "#     # Compute the Continuous Wavelet Transform\n",
    "#     coefficients, _ = pywt.cwt(predictions, scales, wavelet)\n",
    "\n",
    "#     # For simplicity, we use the mean of the coefficients across all scales as the wavelength feature\n",
    "#     wavelength = np.mean(coefficients, axis=0)\n",
    "\n",
    "#     return wavelength\n",
    "\n",
    "# def find_entry_exit(predictions, window_short=5, window_long=10, wavelet='gaus2'):\n",
    "#     \"\"\"Find entry and exit points based on the wavelength of predictions.\"\"\"\n",
    "#     # Convert the predictions to their wavelength using CWT\n",
    "#     wavelength = calculate_wavelength(predictions, wavelet)\n",
    "\n",
    "#     # Convert to pandas Series for rolling window calculations\n",
    "#     wavelength_series = pd.Series(wavelength)\n",
    "\n",
    "#     # Calculate short-term and long-term moving averages on the wavelength\n",
    "#     short_ma = wavelength_series.rolling(window=window_short).mean()\n",
    "#     long_ma = wavelength_series.rolling(window=window_long).mean()\n",
    "\n",
    "#     # Generate signals: 1 for buy, -1 for sell\n",
    "#     buy_signal = (short_ma > long_ma) & (short_ma.shift(1) <= long_ma.shift(1))\n",
    "#     sell_signal = (short_ma < long_ma) & (short_ma.shift(1) >= long_ma.shift(1))\n",
    "\n",
    "#     return buy_signal, sell_signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_rsi(prices, window=14):\n",
    "#     \"\"\"Calculate Relative Strength Index (RSI) for the given prices.\"\"\"\n",
    "#     delta = prices.diff(1)  # Price changes\n",
    "#     gain = delta.where(delta > 0, 0)  # Positive gains\n",
    "#     loss = -delta.where(delta < 0, 0)  # Negative losses\n",
    "\n",
    "#     avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "#     avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "#     rs = avg_gain / avg_loss  # Relative Strength\n",
    "#     rsi = 100 - (100 / (1 + rs))  # RSI Formula\n",
    "\n",
    "#     return rsi\n",
    "\n",
    "# def find_entry_exit(predictions, rsi_window=14, buy_threshold=40, sell_threshold=60):\n",
    "#     \"\"\"Find entry and exit points based on adjusted RSI thresholds.\"\"\"\n",
    "#     prices = pd.Series(predictions.ravel())  # Convert predictions to a Pandas Series\n",
    "\n",
    "#     # Calculate RSI\n",
    "#     rsi = calculate_rsi(prices, window=rsi_window)\n",
    "\n",
    "#     # Generate buy (RSI < buy_threshold) and sell (RSI > sell_threshold) signals\n",
    "#     buy_signal = (rsi < buy_threshold)\n",
    "#     sell_signal = (rsi > sell_threshold)\n",
    "\n",
    "#     return buy_signal, sell_signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the model performance metrics for each outlier\n",
    "# model_perf_list = []\n",
    "\n",
    "# # Loop through each row in the date_ranges DataFrame\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(symbol, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Assign the current outlier_id to the data\n",
    "#     daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     sequence_length = 12\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "\n",
    "#     # Add predictions to the test set for entry/exit calculation\n",
    "#     test_set['predictions'] = predictions_inv\n",
    "\n",
    "#     # Find entry and exit signals\n",
    "#     buy_signal, sell_signal = find_entry_exit(pd.Series(predictions_inv.ravel()))\n",
    "\n",
    "#     # Store the entry and exit dates\n",
    "#     entry_date = test_set['date'][buy_signal].min() if buy_signal.any() else None\n",
    "#     exit_date = test_set['date'][sell_signal].min() if sell_signal.any() else None\n",
    "\n",
    "#     # Initialize profit/loss percentage to None if no trade is possible\n",
    "#     profit_loss_percent = None\n",
    "\n",
    "#     # If we have both entry and exit dates, calculate the profit or loss percentage\n",
    "#     if entry_date is not None and exit_date is not None:\n",
    "#         entry_price = test_set.loc[test_set['date'] == entry_date, 'predictions']\n",
    "#         exit_price = test_set.loc[test_set['date'] == exit_date, 'predictions']\n",
    "\n",
    "#         # Check if entry and exit prices exist (i.e., are not empty)\n",
    "#         if not entry_price.empty and not exit_price.empty:\n",
    "#             entry_price = entry_price.values[0]\n",
    "#             exit_price = exit_price.values[0]\n",
    "#             profit_loss_percent = ((exit_price - entry_price) / entry_price) * 100\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     mse = mean_squared_error(actuals, predictions_inv)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(actuals, predictions_inv)\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "\n",
    "#     # Append the performance metrics to the list, including entry and exit dates and profit/loss percentage\n",
    "#     model_perf_list.append({\n",
    "#         'outlier_id': outlier_id,  # Each range will have a unique outlier_id starting from 1\n",
    "#         'outlier_date': row['outlier_date'],\n",
    "#         'MAE': mae,\n",
    "#         'MSE': mse,\n",
    "#         'RMSE': rmse,\n",
    "#         'MAPE': mape,\n",
    "#         'entry_date': entry_date,\n",
    "#         'exit_date': exit_date,\n",
    "#         'profit_loss_percent': profit_loss_percent\n",
    "#     })\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# model_perf = pd.DataFrame(model_perf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the model performance metrics for each outlier\n",
    "# model_perf_list = []\n",
    "\n",
    "# # Loop through each row in the date_ranges DataFrame\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(symbol, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Assign the current outlier_id to the data\n",
    "#     daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     sequence_length = 12\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "\n",
    "#     # Step 1: Detect a Trend Reversal (Checking if prices are declining for 5 consecutive days and then increasing)\n",
    "#     trend_reversal_detected = False\n",
    "#     entry_date = None\n",
    "#     entry_price = None\n",
    "#     exit_date = None\n",
    "#     exit_price = None\n",
    "#     volatility_threshold = 0.05  # Threshold for detecting a volatility spike\n",
    "#     low_volatility = True\n",
    "\n",
    "#     for i in range(5, forecast_steps):\n",
    "#         # Calculate price changes\n",
    "#         past_5_days_change = np.mean(predictions_inv[i-5:i]) - np.mean(predictions_inv[i-10:i-5])\n",
    "#         future_change = np.mean(predictions_inv[i:i+5]) - np.mean(predictions_inv[i-5:i])\n",
    "\n",
    "#         # Check for a trend reversal\n",
    "#         if past_5_days_change < 0 and future_change > 0 and not trend_reversal_detected:\n",
    "#             entry_date = test_set.iloc[i]['date']\n",
    "#             entry_price = predictions_inv[i][0]\n",
    "#             trend_reversal_detected = True\n",
    "#             print(f\"Trend reversal detected on {entry_date}\")\n",
    "\n",
    "#         # Step 2: Check for volatility (If volatility increases, it's a signal to exit)\n",
    "#         if trend_reversal_detected:\n",
    "#             volatility = np.std(predictions_inv[i:i+5])  # Volatility is measured by the standard deviation of the next 5 days\n",
    "#             if volatility > volatility_threshold and low_volatility:\n",
    "#                 exit_date = test_set.iloc[i]['date']\n",
    "#                 exit_price = predictions_inv[i][0]\n",
    "#                 print(f\"Volatility spike detected on {exit_date}. Exiting trade.\")\n",
    "#                 break\n",
    "#             low_volatility = volatility <= volatility_threshold\n",
    "\n",
    "#     # Step 3: If no exit detected by volatility, exit at the last prediction\n",
    "#     if exit_date is None:\n",
    "#         exit_date = test_set.iloc[-1]['date']\n",
    "#         exit_price = predictions_inv[-1][0]\n",
    "#         print(f\"No volatility spike detected. Exiting trade on {exit_date}\")\n",
    "\n",
    "#     # Calculate profit or loss\n",
    "#     if entry_price is not None and exit_price is not None:\n",
    "#         profit_loss = exit_price - entry_price\n",
    "#     else:\n",
    "#         profit_loss = None\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     mse = mean_squared_error(actuals, predictions_inv)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(actuals, predictions_inv)\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "\n",
    "#     # Append the performance metrics to the list, including entry/exit dates and profit/loss\n",
    "#     model_perf_list.append({\n",
    "#         'outlier_id': outlier_id,\n",
    "#         'outlier_date': row['outlier_date'],\n",
    "#         'entry_date': entry_date,\n",
    "#         'exit_date': exit_date,\n",
    "#         'profit_loss': profit_loss,\n",
    "#         'MAE': mae,\n",
    "#         'MSE': mse,\n",
    "#         'RMSE': rmse,\n",
    "#         'MAPE': mape\n",
    "#     })\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# model_perf = pd.DataFrame(model_perf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Placeholder function to get LSTM predictions (integrating your LSTM logic)\n",
    "# def get_lstm_prediction(daily_data, sequence_length=12):\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "    \n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "#     last_observed_price = daily_data['c'].values[-1]\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "\n",
    "#     # Calculate evaluation metrics\n",
    "#     mse = mean_squared_error(actuals, predictions_inv)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(actuals, predictions_inv)\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "\n",
    "#     return {\n",
    "#         'predictions': predictions_inv,\n",
    "#         'mse': mse,\n",
    "#         'rmse': rmse,\n",
    "#         'mae': mae,\n",
    "#         'mape': mape,\n",
    "#         'predicted_trend': 'up' if predictions_inv.mean() > last_observed_price else 'down',\n",
    "#         'predicted_volatility': np.std(predictions_inv)\n",
    "#     }\n",
    "\n",
    "# # Function to adjust position size based on volatility\n",
    "# def adjust_position_size(volatility):\n",
    "#     if volatility < 0.2:\n",
    "#         return 1.0  # Full position size for low volatility\n",
    "#     elif 0.2 <= volatility <= 0.5:\n",
    "#         return 0.75  # Moderate position size\n",
    "#     else:\n",
    "#         return 0.5  # Reduce position size for high volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define tolerance level and max search days for recovery\n",
    "# tolerance = 0.005\n",
    "# max_days = 10\n",
    "\n",
    "# # Create a DataFrame to store trade results\n",
    "# trade_data = pd.DataFrame(columns=[\n",
    "#     'Outlier Date', 'Entry Date', 'Entry Price', 'Exit Date', 'Exit Price', 'Days Held', 'Profit', 'Volatility Spike Date', 'Position Size'\n",
    "# ])\n",
    "\n",
    "# # Loop through each outlier in date_ranges\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(symbol, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {idx + 1}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Get LSTM predictions and forecast\n",
    "#     lstm_result = get_lstm_prediction(daily_data)\n",
    "\n",
    "#     if lstm_result['predicted_trend'] == 'up':\n",
    "#         # Entry point: outlier date\n",
    "#         entry_date = outlier_date_co\n",
    "#         entry_price = daily_data.loc[daily_data['date'] == outlier_date_co, 'c'].values[0]\n",
    "#         predicted_volatility = lstm_result['predicted_volatility']\n",
    "#         position_size = adjust_position_size(predicted_volatility)\n",
    "\n",
    "#         # Search for exit point based on volatility spikes or recovery within max_days\n",
    "#         future_data = daily_data[daily_data['date'] > outlier_date_co].head(max_days)\n",
    "#         exit_date = None\n",
    "#         exit_price = None\n",
    "#         days_held = 0\n",
    "#         profit = 0\n",
    "#         volatility_spike_date = None\n",
    "\n",
    "#         for _, future_day in future_data.iterrows():\n",
    "#             future_date = future_day['date']\n",
    "#             future_close = future_day['c']\n",
    "#             future_volatility = lstm_result['predicted_volatility']  # You can improve this with dynamic prediction\n",
    "\n",
    "#             # Volatility spike detected\n",
    "#             if future_volatility > 0.5:\n",
    "#                 volatility_spike_date = future_date\n",
    "#                 exit_date = future_date\n",
    "#                 exit_price = future_close\n",
    "#                 days_held = len(pd.bdate_range(start=entry_date, end=exit_date))\n",
    "#                 profit = (exit_price - entry_price) * position_size\n",
    "#                 break\n",
    "\n",
    "#             # Check if recovery within tolerance\n",
    "#             if np.abs(future_close - entry_price) <= tolerance * entry_price:\n",
    "#                 exit_date = future_date\n",
    "#                 exit_price = future_close\n",
    "#                 days_held = len(pd.bdate_range(start=entry_date, end=exit_date))\n",
    "#                 profit = (exit_price - entry_price) * position_size\n",
    "#                 break\n",
    "\n",
    "#         # Default exit if no volatility spike or recovery\n",
    "#         if exit_date is None:\n",
    "#             last_day = future_data.iloc[-1]\n",
    "#             exit_date = last_day['date']\n",
    "#             exit_price = last_day['c']\n",
    "#             days_held = max_days\n",
    "#             profit = (exit_price - entry_price) * position_size\n",
    "\n",
    "#         # Store the result\n",
    "#         trade_info = {\n",
    "#             'Outlier Date': outlier_date_co,\n",
    "#             'Entry Date': entry_date,\n",
    "#             'Entry Price': entry_price,\n",
    "#             'Exit Date': exit_date,\n",
    "#             'Exit Price': exit_price,\n",
    "#             'Days Held': days_held,\n",
    "#             'Profit': profit,\n",
    "#             'Volatility Spike Date': volatility_spike_date,\n",
    "#             'Position Size': position_size\n",
    "#         }\n",
    "#         trade_data = pd.concat([trade_data, pd.DataFrame([trade_info])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the model performance metrics for each outlier\n",
    "# trade_results_list = []\n",
    "\n",
    "# # Define the threshold for entry (0.05% profit target)\n",
    "# profit_threshold = 0.0005  # 0.05% in decimal form\n",
    "\n",
    "# # Loop through each row in the date_ranges DataFrame\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(pair, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Assign the current outlier_id to the data\n",
    "#     daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     sequence_length = 12\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "\n",
    "#     # Trade logic to find entry and exit points\n",
    "#     entry_price = None\n",
    "#     exit_price = None\n",
    "#     entry_date = None\n",
    "#     exit_date = None\n",
    "#     profit_loss = 0\n",
    "#     days_held = 0\n",
    "\n",
    "#     for i in range(1, len(predictions_inv)):\n",
    "#         predicted_change = (predictions_inv[i] - predictions_inv[i - 1]) / predictions_inv[i - 1]\n",
    "#         actual_change = (actuals[i] - actuals[i - 1]) / actuals[i - 1]\n",
    "\n",
    "#         # Entry condition: Price predicted to increase by at least 0.05%\n",
    "#         if entry_price is None and predicted_change >= profit_threshold:\n",
    "#             entry_price = actuals[i - 1]\n",
    "#             entry_date = test_set['date'].iloc[i - 1]\n",
    "        \n",
    "#         # Exit condition: Trend reversal (predicted change turns negative)\n",
    "#         elif entry_price is not None and predicted_change < 0:\n",
    "#             exit_price = actuals[i - 1]\n",
    "#             exit_date = test_set['date'].iloc[i - 1]\n",
    "#             profit_loss = (exit_price - entry_price) * 100 / entry_price  \n",
    "#             days_held = (exit_date - entry_date).days\n",
    "#             break\n",
    "    \n",
    "#     # If no reversal is detected, exit at the last available date\n",
    "#     if entry_price is not None and exit_price is None:\n",
    "#         exit_price = actuals[-1]\n",
    "#         exit_date = test_set['date'].iloc[-1]\n",
    "#         profit_loss = (exit_price - entry_price) * 100 / entry_price \n",
    "#         days_held = (exit_date - entry_date).days\n",
    "\n",
    "#     # Append trade results if entry and exit occurred\n",
    "#     if entry_price is not None and exit_price is not None:\n",
    "#         trade_results_list.append({\n",
    "#             'Outlier Date': outlier_date_co,\n",
    "#             'Entry Date': entry_date,\n",
    "#             'Entry Price': entry_price,\n",
    "#             'Exit Date': exit_date,\n",
    "#             'Exit Price': exit_price,\n",
    "#             'Days Held': days_held,\n",
    "#             'Profit/Loss': profit_loss\n",
    "#         })\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# trade_results_df = pd.DataFrame(trade_results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Startegy 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty list to store the model performance metrics for each outlier\n",
    "# trade_results_list = []\n",
    "\n",
    "# # Define the profit threshold for entry (0.01% profit target)\n",
    "# profit_threshold = 0.1  # 0.1% in decimal form\n",
    "\n",
    "# # Loop through each row in the date_ranges DataFrame\n",
    "# for idx, row in date_ranges.iterrows():\n",
    "#     outlier_id = idx + 1  # Assign a unique outlier_id for each iteration (starting with 1)\n",
    "#     start_date_co = pd.Timestamp(row['start_date'])\n",
    "#     end_date_co = pd.Timestamp(row['end_date']) + pd.Timedelta(days=1)\n",
    "#     outlier_date_co = pd.Timestamp(row['outlier_date'])\n",
    "\n",
    "#     # Fetch and process daily data for the current range\n",
    "#     daily_data = fetch_and_process_daily_data(pair, start_date_co, end_date_co, api_key)\n",
    "\n",
    "#     if daily_data is None:\n",
    "#         print(f\"No data fetched for outlier_id: {outlier_id}\")\n",
    "#         continue  # Skip to the next iteration if no data\n",
    "\n",
    "#     # Assign the current outlier_id to the data\n",
    "#     daily_data['outlier_id'] = outlier_id\n",
    "\n",
    "#     # Filter out weekends\n",
    "#     daily_data = daily_data[~daily_data['date'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "#     # Sort data by date\n",
    "#     daily_data = daily_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "#     # Fill missing values\n",
    "#     daily_data.fillna(method='bfill', inplace=True)\n",
    "#     daily_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#     # Split the dataset into train and test sets\n",
    "#     train_set = daily_data.iloc[:15].reset_index(drop=True)\n",
    "#     test_set = daily_data.iloc[15:].reset_index(drop=True)\n",
    "\n",
    "#     # Normalize the data using only the training data\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_set[[\"c\"]])\n",
    "\n",
    "#     # Prepare data for LSTM model\n",
    "#     sequence_length = 12\n",
    "#     train_generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "\n",
    "#     # Define and compile LSTM model\n",
    "#     model = Sequential([\n",
    "#         LSTM(64, activation='relu', input_shape=(sequence_length, 1)),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_generator, epochs=100, verbose=0)\n",
    "\n",
    "#     # Prepare the last sequence for forecasting\n",
    "#     last_sequence = train_scaled[-sequence_length:]\n",
    "\n",
    "#     # Forecast the next steps\n",
    "#     forecast_steps = len(test_set)\n",
    "#     predictions_scaled = []\n",
    "#     for _ in range(forecast_steps):\n",
    "#         last_sequence_reshaped = last_sequence.reshape((1, sequence_length, 1))\n",
    "#         next_step_pred = model.predict(last_sequence_reshaped, verbose=0)\n",
    "#         predictions_scaled.append(next_step_pred.ravel()[0])\n",
    "#         last_sequence = np.roll(last_sequence, -1)\n",
    "#         last_sequence[-1] = next_step_pred\n",
    "\n",
    "#     # Inverse transform predictions\n",
    "#     predictions_inv = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "\n",
    "#     # Actual values for comparison\n",
    "#     actuals = test_set[\"c\"].values[:forecast_steps]\n",
    "    \n",
    "# \t# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "#     mse = mean_squared_error(actuals, predictions_inv)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mape = mean_absolute_percentage_error(actuals, predictions_inv)\n",
    "#     accuracy = np.round(100 - (mape * 100), 2)\n",
    "\n",
    "#     # Trade logic to find entry and exit points\n",
    "#     entry_price = None\n",
    "#     exit_price = None\n",
    "#     entry_date = None\n",
    "#     exit_date = None\n",
    "#     profit_loss = 0\n",
    "#     days_held = 0\n",
    "#     trade_initiated = False\n",
    "\n",
    "#     for i in range(1, len(predictions_inv)):\n",
    "#         predicted_change = (predictions_inv[i] - predictions_inv[i - 1]) * 100 / predictions_inv[i - 1]\n",
    "#         actual_change = (actuals[i] - actuals[i - 1]) * 100 / actuals[i - 1]\n",
    "\n",
    "#         # Entry condition: Price predicted to increase by at least 0.05% consistently for a few days\n",
    "#         if entry_price is None and predicted_change >= profit_threshold:\n",
    "#             # Check if future predictions show a continued upward trend\n",
    "#             future_predictions = predictions_inv[i:i + 2]  # Look at the next 3 predicted values\n",
    "#             if all((future_predictions[j] - future_predictions[j - 1]) * 100 / future_predictions[j - 1] >= profit_threshold for j in range(1, len(future_predictions))):\n",
    "#                 entry_price = actuals[i - 1]\n",
    "#                 entry_date = test_set['date'].iloc[i - 1]\n",
    "#                 trade_initiated = True\n",
    "#             else:\n",
    "#                 trade_initiated = False  # Skip trade if no consistent upward trend\n",
    "\n",
    "#         # Exit condition: Trend reversal (predicted change turns negative)\n",
    "#         elif entry_price is not None and predicted_change < 0:\n",
    "#             exit_price = actuals[i - 1]\n",
    "#             exit_date = test_set['date'].iloc[i - 1]\n",
    "#             profit_loss = (exit_price - entry_price) * 100 / entry_price\n",
    "#             days_held = (exit_date - entry_date).days\n",
    "#             break\n",
    "\n",
    "#     # If no reversal is detected, exit at the last available date\n",
    "#     if trade_initiated and entry_price is not None and exit_price is None:\n",
    "#         exit_price = actuals[-1]\n",
    "#         exit_date = test_set['date'].iloc[-1]\n",
    "#         profit_loss = (exit_price - entry_price) * 100 / entry_price\n",
    "#         days_held = (exit_date - entry_date).days\n",
    "\n",
    "#     # Append trade results only if there was a valid trade (i.e., entry and exit with profit > 0.1%) and rmse < 0.05\n",
    "#     if trade_initiated and entry_price is not None and exit_price is not None and profit_loss > profit_threshold and rmse < 0.0:\n",
    "#         trade_results_list.append({\n",
    "#             'Outlier Date': outlier_date_co,\n",
    "#             'Outlier Price': row['outlier_price'],\n",
    "#             'Entry Date': entry_date,\n",
    "#             'Entry Price': entry_price,\n",
    "#             'Exit Date': exit_date,\n",
    "#             'Exit Price': exit_price,\n",
    "#             'Days Held': days_held,\n",
    "#             'Profit': profit_loss,\n",
    "#             'model_RMSE': rmse,\n",
    "#             'model_accuracy': accuracy\n",
    "#         })\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame\n",
    "# trade_results_df = pd.DataFrame(trade_results_list)\n",
    "\n",
    "# # Show the final dataframe with trade results\n",
    "# trade_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
